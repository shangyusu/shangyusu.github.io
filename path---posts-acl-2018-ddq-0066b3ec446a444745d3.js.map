{"version":3,"sources":["webpack:///path---posts-acl-2018-ddq-0066b3ec446a444745d3.js","webpack:///./.cache/json/posts-acl-2018-ddq.json"],"names":["webpackJsonp","426","module","exports","data","site","siteMetadata","title","subtitle","copyright","author","name","twitter","disqusShortname","url","markdownRemark","id","html","fields","tagSlugs","frontmatter","tags","date","description","conference","pathContext","slug"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,MAAQC,cAAgBC,MAAA,cAAAC,SAAA,wIAAAC,UAAA,yBAAAC,QAAwNC,KAAA,cAAAC,QAAA,KAAmCC,gBAAA,GAAAC,IAAA,8BAAyDC,gBAAmBC,GAAA,+HAAAC,KAAA,g1DAAi8BC,QAAsjCC,UAAA,6EAAwFC,aAAgBb,MAAA,iFAAAc,MAAA,uDAAAC,KAAA,2BAAAC,YAAA,oFAAAC,WAAA,cAAsTC,aAAgBC,KAAA","file":"path---posts-acl-2018-ddq-0066b3ec446a444745d3.js","sourcesContent":["webpackJsonp([237107909507164],{\n\n/***/ 426:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"site\":{\"siteMetadata\":{\"title\":\"Shang-Yu Su\",\"subtitle\":\"PhD student in National Taiwan University, research interests cover Deep Learning, Natural Language Processing, and dialogue systems.\",\"copyright\":\"© All rights reserved.\",\"author\":{\"name\":\"Shang-Yu Su\",\"twitter\":\"#\"},\"disqusShortname\":\"\",\"url\":\"https://www.shangyusu.com\"}},\"markdownRemark\":{\"id\":\"/Users/ShangYu/Desktop/github/shangyusu.github.io/src/pages/articles/ACL2018-DDQ/index.md absPath of file >>> MarkdownRemark\",\"html\":\"<p><b>This paper is published in Proceedings of The 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018).</b></p>\\n<p>Full paper:\\n<a href=\\\"/acl-2018-deep-4efbe3c99de146207ef31cd7523496ed.pdf\\\" target=\\\"_blank\\\">Here</a>,\\n<a href=\\\"https://arxiv.org/abs/1801.06176\\\" target=\\\"_blank\\\">arXiv</a></p>\\n<p>Code:\\n<a href=\\\"https://github.com/MiuLab/DDQ\\\" target=\\\"_blank\\\">GitHub</a></p>\\n<p>Cite the paper:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">@article{peng2018integrating,\\n  title={Integrating planning for task-completion dialogue policy learning},\\n  author={Peng, Baolin and Li, Xiujun and Gao, Jianfeng and Liu, Jingjing and Wong, Kam-Fai and Su, Shang-Yu},\\n  journal={arXiv preprint arXiv:1801.06176},\\n  year={2018}\\n}</code></pre>\\n      </div>\\n<p>Training a task-completion dialogue agent via reinforcement learning (RL) is costly because it requires many interactions with real users. One common alternative is to use a user simulator. However, a user simulator usually lacks the language complexity of human interlocutors and the biases in its design may tend to degrade the agent. To address these issues, we present Deep Dyna-Q, which to our knowledge is the first deep RL framework that integrates planning for task-completion dialogue policy learning. We incorporate into the dialogue agent a model of the environment, referred to as the world model, to mimic real user response and generate simulated experience. During dialogue policy learning, the world model is constantly updated with real user experience to approach real user behavior, and in turn, the dialogue agent is optimized using both real experience and simulated experience. The effectiveness of our approach is demonstrated on a movie-ticket booking task in both simulated and human-in-the-loop settings.</p>\",\"fields\":{\"tagSlugs\":[\"/tags/dialogue/\",\"/tags/dialogue-policy/\",\"/tags/reinforcement-learning/\"]},\"frontmatter\":{\"title\":\"Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning\",\"tags\":[\"Dialogue\",\"Dialogue Policy\",\"Reinforcement Learning\"],\"date\":\"2018-02-10T22:40:32.169Z\",\"description\":\"Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu, Kam-Fai Wong, and Shang-Yu Su\",\"conference\":\"ACL 2018\"}}},\"pathContext\":{\"slug\":\"/posts/acl2018-ddq/\"}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---posts-acl-2018-ddq-0066b3ec446a444745d3.js","module.exports = {\"data\":{\"site\":{\"siteMetadata\":{\"title\":\"Shang-Yu Su\",\"subtitle\":\"PhD student in National Taiwan University, research interests cover Deep Learning, Natural Language Processing, and dialogue systems.\",\"copyright\":\"© All rights reserved.\",\"author\":{\"name\":\"Shang-Yu Su\",\"twitter\":\"#\"},\"disqusShortname\":\"\",\"url\":\"https://www.shangyusu.com\"}},\"markdownRemark\":{\"id\":\"/Users/ShangYu/Desktop/github/shangyusu.github.io/src/pages/articles/ACL2018-DDQ/index.md absPath of file >>> MarkdownRemark\",\"html\":\"<p><b>This paper is published in Proceedings of The 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018).</b></p>\\n<p>Full paper:\\n<a href=\\\"/acl-2018-deep-4efbe3c99de146207ef31cd7523496ed.pdf\\\" target=\\\"_blank\\\">Here</a>,\\n<a href=\\\"https://arxiv.org/abs/1801.06176\\\" target=\\\"_blank\\\">arXiv</a></p>\\n<p>Code:\\n<a href=\\\"https://github.com/MiuLab/DDQ\\\" target=\\\"_blank\\\">GitHub</a></p>\\n<p>Cite the paper:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">@article{peng2018integrating,\\n  title={Integrating planning for task-completion dialogue policy learning},\\n  author={Peng, Baolin and Li, Xiujun and Gao, Jianfeng and Liu, Jingjing and Wong, Kam-Fai and Su, Shang-Yu},\\n  journal={arXiv preprint arXiv:1801.06176},\\n  year={2018}\\n}</code></pre>\\n      </div>\\n<p>Training a task-completion dialogue agent via reinforcement learning (RL) is costly because it requires many interactions with real users. One common alternative is to use a user simulator. However, a user simulator usually lacks the language complexity of human interlocutors and the biases in its design may tend to degrade the agent. To address these issues, we present Deep Dyna-Q, which to our knowledge is the first deep RL framework that integrates planning for task-completion dialogue policy learning. We incorporate into the dialogue agent a model of the environment, referred to as the world model, to mimic real user response and generate simulated experience. During dialogue policy learning, the world model is constantly updated with real user experience to approach real user behavior, and in turn, the dialogue agent is optimized using both real experience and simulated experience. The effectiveness of our approach is demonstrated on a movie-ticket booking task in both simulated and human-in-the-loop settings.</p>\",\"fields\":{\"tagSlugs\":[\"/tags/dialogue/\",\"/tags/dialogue-policy/\",\"/tags/reinforcement-learning/\"]},\"frontmatter\":{\"title\":\"Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning\",\"tags\":[\"Dialogue\",\"Dialogue Policy\",\"Reinforcement Learning\"],\"date\":\"2018-02-10T22:40:32.169Z\",\"description\":\"Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu, Kam-Fai Wong, and Shang-Yu Su\",\"conference\":\"ACL 2018\"}}},\"pathContext\":{\"slug\":\"/posts/acl2018-ddq/\"}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/posts-acl-2018-ddq.json\n// module id = 426\n// module chunks = 237107909507164"],"sourceRoot":""}