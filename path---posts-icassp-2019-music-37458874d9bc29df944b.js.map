{"version":3,"sources":["webpack:///path---posts-icassp-2019-music-37458874d9bc29df944b.js","webpack:///./.cache/json/posts-icassp-2019-music.json"],"names":["webpackJsonp","450","module","exports","data","site","siteMetadata","title","subtitle","copyright","author","name","twitter","disqusShortname","url","markdownRemark","id","html","fields","tagSlugs","frontmatter","tags","date","description","conference","pathContext","slug"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,MAAQC,cAAgBC,MAAA,cAAAC,SAAA,wIAAAC,UAAA,yBAAAC,QAAwNC,KAAA,cAAAC,QAAA,KAAmCC,gBAAA,GAAAC,IAAA,8BAAyDC,gBAAmBC,GAAA,oIAAAC,KAAA,iqCAAAC,QAAm0CC,UAAA,wEAAmFC,aAAgBb,MAAA,gFAAAc,MAAA,kDAAAC,KAAA,0BAAAC,YAAA,6EAAAC,WAAA,gBAA0SC,aAAgBC,KAAA","file":"path---posts-icassp-2019-music-37458874d9bc29df944b.js","sourcesContent":["webpackJsonp([18437839863029],{\n\n/***/ 450:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"site\":{\"siteMetadata\":{\"title\":\"Shang-Yu Su\",\"subtitle\":\"PhD student at National Taiwan University, research interests cover Deep Learning, Natural Language Processing, and dialogue systems.\",\"copyright\":\"© All rights reserved.\",\"author\":{\"name\":\"Shang-Yu Su\",\"twitter\":\"#\"},\"disqusShortname\":\"\",\"url\":\"https://www.shangyusu.com\"}},\"markdownRemark\":{\"id\":\"/Users/ShangYu/Desktop/github/shangyusu.github.io/src/pages/articles/ICASSP2019-music/index.md absPath of file >>> MarkdownRemark\",\"html\":\"<p><b>This paper is published in the in Proceedings of The 44th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2019), Brighton, U.K., May 12-17, 2019. IEEE.</b></p>\\n<p>Full paper:\\n<a href=\\\"/ICASSP_2019___Music_Generation-81c090fb9606994fe2125873e57dbc71.pdf\\\" target=\\\"_blank\\\">Here</a>,\\n<a href=\\\"https://arxiv.org/abs/1811.00162\\\" target=\\\"_blank\\\">arXiv</a></p>\\n<p>Automatic melody generation has been a long-time aspiration for both AI researchers and musicians.\\nHowever, learning to generate euphonious melodies has turned out to be highly challenging.\\nThis paper introduces 1) a new variant of variational autoencoder (VAE), where the model structure is designed in a modularized manner in order to model polyphonic and dynamic music with domain knowledge, and 2) a hierarchical encoding/decoding strategy, which explicitly models the dependency between melodic features. The proposed framework is capable of generating distinct melodies that sounds natural, and the experiments for evaluating generated music clips show that the proposed model outperforms the baselines in human evaluation. (The first three authors contributed equally.)</p>\",\"fields\":{\"tagSlugs\":[\"/tags/music-information-retrieval/\",\"/tags/generation/\",\"/tags/vae/\"]},\"frontmatter\":{\"title\":\"Modeling Melodic Feature Dependency with Modularized Variational Auto-Encoder\",\"tags\":[\"Music Information Retrieval\",\"Generation\",\"VAE\"],\"date\":\"2019-02-7T22:40:32.169Z\",\"description\":\"Yu-An Wang*, Yu-Kai Huang*, Tzu-Chuan Lin*, Shang-Yu Su, and Yun-Nung Chen\",\"conference\":\"ICASSP2019\"}}},\"pathContext\":{\"slug\":\"/posts/icassp2019-music/\"}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---posts-icassp-2019-music-37458874d9bc29df944b.js","module.exports = {\"data\":{\"site\":{\"siteMetadata\":{\"title\":\"Shang-Yu Su\",\"subtitle\":\"PhD student at National Taiwan University, research interests cover Deep Learning, Natural Language Processing, and dialogue systems.\",\"copyright\":\"© All rights reserved.\",\"author\":{\"name\":\"Shang-Yu Su\",\"twitter\":\"#\"},\"disqusShortname\":\"\",\"url\":\"https://www.shangyusu.com\"}},\"markdownRemark\":{\"id\":\"/Users/ShangYu/Desktop/github/shangyusu.github.io/src/pages/articles/ICASSP2019-music/index.md absPath of file >>> MarkdownRemark\",\"html\":\"<p><b>This paper is published in the in Proceedings of The 44th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2019), Brighton, U.K., May 12-17, 2019. IEEE.</b></p>\\n<p>Full paper:\\n<a href=\\\"/ICASSP_2019___Music_Generation-81c090fb9606994fe2125873e57dbc71.pdf\\\" target=\\\"_blank\\\">Here</a>,\\n<a href=\\\"https://arxiv.org/abs/1811.00162\\\" target=\\\"_blank\\\">arXiv</a></p>\\n<p>Automatic melody generation has been a long-time aspiration for both AI researchers and musicians.\\nHowever, learning to generate euphonious melodies has turned out to be highly challenging.\\nThis paper introduces 1) a new variant of variational autoencoder (VAE), where the model structure is designed in a modularized manner in order to model polyphonic and dynamic music with domain knowledge, and 2) a hierarchical encoding/decoding strategy, which explicitly models the dependency between melodic features. The proposed framework is capable of generating distinct melodies that sounds natural, and the experiments for evaluating generated music clips show that the proposed model outperforms the baselines in human evaluation. (The first three authors contributed equally.)</p>\",\"fields\":{\"tagSlugs\":[\"/tags/music-information-retrieval/\",\"/tags/generation/\",\"/tags/vae/\"]},\"frontmatter\":{\"title\":\"Modeling Melodic Feature Dependency with Modularized Variational Auto-Encoder\",\"tags\":[\"Music Information Retrieval\",\"Generation\",\"VAE\"],\"date\":\"2019-02-7T22:40:32.169Z\",\"description\":\"Yu-An Wang*, Yu-Kai Huang*, Tzu-Chuan Lin*, Shang-Yu Su, and Yun-Nung Chen\",\"conference\":\"ICASSP2019\"}}},\"pathContext\":{\"slug\":\"/posts/icassp2019-music/\"}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/posts-icassp-2019-music.json\n// module id = 450\n// module chunks = 18437839863029"],"sourceRoot":""}