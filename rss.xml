<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Shang-Yu Su]]></title><description><![CDATA[PhD student in National Taiwan University, research interests cover Deep Learning, Natural Language Processing, and dialogue systems.]]></description><link>https://www.shangyusu.com</link><generator>RSS for Node</generator><lastBuildDate>Sat, 01 Jun 2019 06:27:29 GMT</lastBuildDate><item><title><![CDATA[Dual Supervised Learning for Natural Language Understanding and Generation]]></title><description><![CDATA[Shang-Yu Su, Chao-Wei Huang, and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/posts/acl2019-dsl/</link><guid isPermaLink="false">https://www.shangyusu.com/posts/acl2019-dsl/</guid><pubDate>Fri, 17 May 2019 22:40:32 GMT</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in Proceedings of The 57th Annual Meeting of the Association for Computational Linguistics (ACL).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/ACL_2019___Dual_Supervised_Learning2-ab700554e0ebde63ca74abf3a393d37e.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;,
&lt;a href=&quot;https://arxiv.org/abs/1905.06196&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Natural language understanding (NLU) and natural language generation (NLG) are both critical research topics in the NLP field.
Natural language understanding is to extract the core semantic meaning from the given utterances, while natural language generation is opposite, of which the goal is to construct corresponding sentences based on the given semantics.
However, such dual relationship has not been investigated in the literature.
This paper proposes a new learning framework for language understanding and generation on top of dual supervised learning, providing a way to exploit the duality.
The preliminary experiments show that the proposed approach boosts the performance for both tasks. &lt;/p&gt;</content:encoded></item><item><title><![CDATA[Modeling Melodic Feature Dependency with Modularized Variational Auto-Encoder]]></title><description><![CDATA[Yu-An Wang*, Yu-Kai Huang*, Tzu-Chuan Lin*, Shang-Yu Su, and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/posts/icassp2019-music/</link><guid isPermaLink="false">https://www.shangyusu.com/posts/icassp2019-music/</guid><pubDate>Invalid Date</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in the in Proceedings of The 44th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2019), Brighton, U.K., May 12-17, 2019. IEEE.&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/ICASSP_2019___Music_Generation-81c090fb9606994fe2125873e57dbc71.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;,
&lt;a href=&quot;https://arxiv.org/abs/1811.00162&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Automatic melody generation has been a long-time aspiration for both AI researchers and musicians.
However, learning to generate euphonious melodies has turned out to be highly challenging.
This paper introduces 1) a new variant of variational autoencoder (VAE), where the model structure is designed in a modularized manner in order to model polyphonic and dynamic music with domain knowledge, and 2) a hierarchical encoding/decoding strategy, which explicitly models the dependency between melodic features. The proposed framework is capable of generating distinct melodies that sounds natural, and the experiments for evaluating generated music clips show that the proposed model outperforms the baselines in human evaluation. (The first three authors contributed equally.)&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Compound Variational Auto-Encoder]]></title><description><![CDATA[Shang-Yu Su, Shan-Wei Lin, and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/posts/icassp2019-comvae/</link><guid isPermaLink="false">https://www.shangyusu.com/posts/icassp2019-comvae/</guid><pubDate>Invalid Date</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in the in Proceedings of The 44th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2019), Brighton, U.K., May 12-17, 2019. IEEE.&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/ICASSP_ComVAE-34ac75152d91d929def8cf5c13d83b62.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Amortized variational inference (AVI) enables efficient training of deep generative models to scale to large datasets. The quality of the approximate inference is determined by various reasons, such as the ability of producing proper variational parameters for each datapoint in the recognition network and whether the variational distribution matches the true posterior, etc.
This paper focuses on the inference sub-optimality of variational auto-encoders (VAE), where the goal is to reduce the difference caused by amortizing the variational distribution parameters over the entire training set instead of optimizing for each training example individually, which is also known as the amortization gap.
This paper extends Bayesian inference in VAE from the latent level to both latent and weight levels by adopting Bayesian neural networks (BNN) in the encoder, so that each datapoint obtains its own distribution for better modeling.
The hybrid design in the proposed compound VAE is empirically demonstrated to be capable of mitigating the amortization gap.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Dynamically Context-Sensitive Time-Decay Attention for Dialogue Modeling]]></title><description><![CDATA[Shang-Yu Su, Pei-Chieh Yuan, and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/posts/icassp2019-cnlu/</link><guid isPermaLink="false">https://www.shangyusu.com/posts/icassp2019-cnlu/</guid><pubDate>Invalid Date</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in the in Proceedings of The 44th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2019), Brighton, U.K., May 12-17, 2019. IEEE.&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/ICASSP_2019___Context_Sensitive_Time_Decay-0592df6fa69ea234e824ec9a1ddf928c.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;,
&lt;a href=&quot;https://arxiv.org/abs/1809.01557&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Spoken language understanding (SLU) is an essential component in conversational systems. Considering that contexts provide informative cues for better understanding, history can be leveraged for contextual SLU. However, most prior work only paid attention to the related content in history utterances and ignored the temporal information. In dialogues, it is intuitive that the most recent utterances are more important than the least recent ones, hence time-aware attention should be in a decaying manner. Therefore, this paper allows the model to automatically learn a time-decay attention function based on the content of each roleâ€™s contexts, which effectively integrates both content-aware and time-aware perspectives and demonstrates remarkable flexibility to complex dialogue contexts. The experiments on the benchmark Dialogue State Tracking Challenge (DSTC4) dataset show that the proposed role-based context-sensitive time-decay attention mechanisms significantly improve the state-of-the-art model for contextual understanding performance. &lt;/p&gt;</content:encoded></item><item><title><![CDATA[Reactive Multi-Stage Feature Fusion for Multimodal Dialogue Modeling]]></title><description><![CDATA[Yi-Ting Yeh, Tzu-Chuan Lin, Hsiao-Hua Cheng, Yi-Hsuan Deng, Shang-Yu Su, and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/posts/aaai2019-dstc7-avsd/</link><guid isPermaLink="false">https://www.shangyusu.com/posts/aaai2019-dstc7-avsd/</guid><pubDate>Invalid Date</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in the 7th Dialog System Technology Challenge (DSTC7) in the Proceedings of Thirty-Third AAAI Conference on Artificial Intelligence (AAAI 2019 - DSTC7).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/AAAI_2019___DSTC_Track_3-734c834eba9b64f63af0fae9f9030aba.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Visual question answering and visual dialogue tasks have been increasingly studied in the multimodal field towards more practical real-world scenarios.
A more challenging task, audio visual scene-aware dialogue (AVSD), is proposed to further advance the technologies that connect audio, vision, and language, which introduces temporal video information and dialogue interactions between a questioner and an answerer.
This paper proposes an intuitive mechanism that fuses features and attention in multiple stages in order to well integrate multimodal features, and the results demonstrate its capability in the experiments.
Also, we apply several state-of-the-art models in other tasks to the AVSD task, and further analyze their generalization across different tasks.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Knowledge-Grounded Response Generation with Deep Attentional Latent-Variable Model]]></title><description><![CDATA[Hao-Tong Ye, Kai-Ling Lo, Shang-Yu Su, and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/posts/aaai2019-dstc7-cvae/</link><guid isPermaLink="false">https://www.shangyusu.com/posts/aaai2019-dstc7-cvae/</guid><pubDate>Invalid Date</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in the 7th Dialog System Technology Challenge (DSTC7) in the Proceedings of Thirty-Third AAAI Conference on Artificial Intelligence (AAAI 2019 - DSTC7).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/AAAI_2019___DSTC_Track_2-1ac5ec1418e3e3049e3e156b0d215230.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;End-to-end dialogue generation has achieved promising results without using handcrafted features and attributes specific for each task and corpus. However, one of the fatal drawbacks in such approaches is that they are unable to generate informative utterances, so it limits their usage from some real-world conversational applications. This paper attempts at generating diverse and informative responses with a variational generation model, which contains a joint attention mechanism conditioning on the information from both dialogue contexts and extra knowledge.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Learning Multi-Level Information for Dialogue Response Selection by Highway Recurrent Transformer]]></title><description><![CDATA[Ting-Rui Chiang, Chao-Wei Huang, Shang-Yu Su, and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/posts/aaai2019-dstc7-hrt/</link><guid isPermaLink="false">https://www.shangyusu.com/posts/aaai2019-dstc7-hrt/</guid><pubDate>Invalid Date</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in the 7th Dialog System Technology Challenge (DSTC7) in the Proceedings of Thirty-Third AAAI Conference on Artificial Intelligence (AAAI 2019 - DSTC7).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/AAAI_2019___DSTC_Track_1_2-1c0c2c056933540e847486427a9bfc64.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;With the increasing research interest in dialogue response generation, there is an emerging branch formulating this task as selecting next sentences, where given the partial dialogue contexts, the goal is to determine the most probable next sentence.
Following the recent success of the Transformer model, this paper proposes (1) a new variant of attention mechanism based on multi-head attention, called highway attention, and (2) a recurrent model based on transformer and the proposed highway attention, so-called Highway Recurrent Transformer.
Experiments on the response selection task in the seventh Dialog System Technology Challenge (DSTC7) show the capability of the proposed model of modeling both utterance-level and dialogue-level information; the effectiveness of each module is further analyzed as well.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[RAP-Net: Recurrent Attention Pooling Networks for Dialogue Response Selection]]></title><description><![CDATA[Chao-Wei Huang, Ting-Rui Chiang, Shang-Yu Su, and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/posts/aaai2019-dstc7-rapnet/</link><guid isPermaLink="false">https://www.shangyusu.com/posts/aaai2019-dstc7-rapnet/</guid><pubDate>Invalid Date</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in the 7th Dialog System Technology Challenge (DSTC7) in the Proceedings of Thirty-Third AAAI Conference on Artificial Intelligence (AAAI 2019 - DSTC7).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/AAAI_2019___DSTC_Track_1-2d7495af163e0096f9714141fdcdc7aa.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The response selection has been an emerging research topic due to the growing interest in dialogue modeling, where the goal of the task is to select an appropriate response for continuing dialogues.
To further push the end-to-end dialogue model toward real-world scenarios, the seventh Dialog System Technology Challenge (DSTC7) proposed a challenge track based on real chatlog datasets.
The competition focuses on dialogue modeling with several advanced characteristics: (1) natural language diversity, (2) capability of precisely selecting a proper response from a large set of candidates or the scenario without any correct answer, and (3) knowledge grounding.
This paper introduces recurrent attention pooling networks (RAP-Net), a novel framework for response selection, which can well estimate the relevance between the dialogue contexts and the candidates.
The proposed RAP-Net is shown to be effective and can be generalize across different datasets and settings in the DSTC7 experiments.
In the future, the proposed model can be evaluated on other retrieval-based tasks to test the model capability of generalization.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Investigating Linguistic Pattern Ordering in Hierarchical Natural Language Generation]]></title><description><![CDATA[Shang-Yu Su, and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/posts/slt2018-hnlg/</link><guid isPermaLink="false">https://www.shangyusu.com/posts/slt2018-hnlg/</guid><pubDate>Invalid Date</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in Proceedings of 7th IEEE Workshop on Spoken Language Technology (SLT 2018).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/slt-2018-hnlg-bd09cbcfc6315cef7fbf9653703534f9.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;,
&lt;a href=&quot;https://arxiv.org/abs/1809.07629&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code:
&lt;a href=&quot;https://github.com/MiuLab/HNLG&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Natural language generation (NLG) is a critical component in spoken dialogue system, which can be divided into two phases: (1) sentence planning: deciding the overall sentence structure, (2) surface realization: determining specific word forms and flattening the sentence structure into a string.
With the rise of deep learning, most modern NLG models are based on a sequence-to-sequence (seq2seq) model, which basically contains an encoder-decoder structure; these NLG models generate sentences from scratch by jointly optimizing sentence planning and surface realization.
However, such simple encoder-decoder architecture usually fail to generate complex and long sentences, because the decoder has difficulty learning all grammar and diction knowledge well.
This paper introduces an NLG model with a hierarchical attentional decoder, where the hierarchy focuses on leveraging linguistic knowledge in a specific order. The experiments show that the proposed method significantly outperforms the traditional seq2seq model with a smaller model size, and the design of the hierarchical attentional decoder can be applied to various NLG systems.
Furthermore, different generation strategies based on linguistic patterns are investigated and analyzed in order to guide future NLG research work.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Discriminative Deep Dyna-Q: Robust Planning for Dialogue Policy Learning]]></title><description><![CDATA[Shang-Yu Su, Xiujun Li, Jianfeng Gao, Jingjing Liu and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/posts/emnlp2018-d3q/</link><guid isPermaLink="false">https://www.shangyusu.com/posts/emnlp2018-d3q/</guid><pubDate>Mon, 13 Aug 2018 22:40:32 GMT</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP 2018).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/emnlp-2018-d3q-b71cd121612e0c757f51ccd99c6b5480.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;,
&lt;a href=&quot;https://arxiv.org/abs/1808.09442&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This paper presents a Discriminative Deep Dyna-Q (D3Q) approach to improving the effectiveness and robustness of DDQ, a recently proposed framework that extends the Dyna-Q algorithm to integrate planning for task-completion dialogue policy learning. To obviate DDQâ€™s high dependency on the quality of simulated experiences, we incorporate an RNN-based discriminator in D3Q to differentiate simulated experience from real user experience in order to control the quality of training data. Experiments show that D3Q significantly outperforms DDQ by controlling the quality of simulated experience used for planning. The effectiveness and robustness of D3Q is further demonstrated in a domain extension setting, where the agentâ€™s capability of adapting to a changing environment is tested.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning]]></title><description><![CDATA[Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu, Kam-Fai Wong, and Shang-Yu Su]]></description><link>https://www.shangyusu.com/posts/acl2018-ddq/</link><guid isPermaLink="false">https://www.shangyusu.com/posts/acl2018-ddq/</guid><pubDate>Sat, 10 Feb 2018 22:40:32 GMT</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in Proceedings of The 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/acl-2018-deep-4efbe3c99de146207ef31cd7523496ed.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;,
&lt;a href=&quot;https://arxiv.org/abs/1801.06176&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code:
&lt;a href=&quot;https://github.com/MiuLab/DDQ&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cite the paper:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;@article{peng2018integrating,
  title={Integrating planning for task-completion dialogue policy learning},
  author={Peng, Baolin and Li, Xiujun and Gao, Jianfeng and Liu, Jingjing and Wong, Kam-Fai and Su, Shang-Yu},
  journal={arXiv preprint arXiv:1801.06176},
  year={2018}
}&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Training a task-completion dialogue agent via reinforcement learning (RL) is costly because it requires many interactions with real users. One common alternative is to use a user simulator. However, a user simulator usually lacks the language complexity of human interlocutors and the biases in its design may tend to degrade the agent. To address these issues, we present Deep Dyna-Q, which to our knowledge is the first deep RL framework that integrates planning for task-completion dialogue policy learning. We incorporate into the dialogue agent a model of the environment, referred to as the world model, to mimic real user response and generate simulated experience. During dialogue policy learning, the world model is constantly updated with real user experience to approach real user behavior, and in turn, the dialogue agent is optimized using both real experience and simulated experience. The effectiveness of our approach is demonstrated on a movie-ticket booking task in both simulated and human-in-the-loop settings.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[How Time Matters: Learning Time-Decay Attention for Contextual Spoken Language Understanding in Dialogues]]></title><description><![CDATA[Shang-Yu Su, Pei-Chieh Yuan, and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/posts/naacl2018-nlu/</link><guid isPermaLink="false">https://www.shangyusu.com/posts/naacl2018-nlu/</guid><pubDate>Sat, 03 Feb 2018 22:40:32 GMT</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in Proceedings of The 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2018).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/naacl-hlt-2018-nlu-camera-ready-17349e002beae6efda18798e5d19707d.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code:
&lt;a href=&quot;https://github.com/MiuLab/Time-Decay-SLU&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cite the paper:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;@inproceedings{su2018how,
  title={How time matters: Learning Time-Decay Attention for Contextual Spoken Language Understanding in Dialogues},
    author={Shang-Yu Su, Pei-Chieh Yuan, and Yun-Nung Chen},
    booktitle={Proceedings of The 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    year={2018}
}&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Spoken language understanding (SLU) is an essential component in conversational systems. Most SLU components treats each utterance independently, and then the following components aggregate the multi-turn information in the separate phases.
In order to avoid error propagation and effectively utilize contexts, prior work leveraged history for contextual SLU.&lt;/p&gt;
&lt;p&gt;However, most previous models only paid attention to the related content in history utterances, ignoring their temporal information.
In the dialogues, it is intuitive that the most recent utterances are more important than the least recent ones, in other words, time-aware attention should be in a decaying manner.&lt;/p&gt;
&lt;p&gt;Therefore, this paper designs and investigates various types of time-decay attention on the sentence-level and speaker-level, and further proposes a flexible universal time-decay attention mechanism.
The experiments on the benchmark Dialogue State Tracking Challenge (DSTC4) dataset show that the proposed time-decay attention mechanisms significantly improve the state-of-the-art model for contextual understanding performance (The source code is at: &lt;a href=&quot;https://github.com/MiuLab/Time-Decay-SLU&quot;&gt;https://github.com/MiuLab/Time-Decay-SLU&lt;/a&gt;).&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Natural Language Generation by Hierarchical Decoding with Linguistic Patterns]]></title><description><![CDATA[Shang-Yu Su*, Kai-Ling Lo*, Yi-Ting Yeh, and Yun-Nung Chen (co-first author)]]></description><link>https://www.shangyusu.com/posts/naacl2018-nlg/</link><guid isPermaLink="false">https://www.shangyusu.com/posts/naacl2018-nlg/</guid><pubDate>Fri, 02 Feb 2018 22:40:32 GMT</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in Proceedings of The 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2018).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/naacl-hlt-2018-nlg-1a01ffc4d8392caed4366ed051fc5565.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;,
&lt;a href=&quot;https://arxiv.org/abs/1808.02747&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code:
&lt;a href=&quot;https://github.com/MiuLab/HNLG&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cite the paper:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;@inproceedings{su2018natural,
  title={Natural Language Generation by Hierarchical Decoding with Linguistic Patterns},
    author={Shang-Yu Su, Kai-Ling Lo, Yi-Ting Yeh, and Yun-Nung Chen},
    booktitle={Proceedings of The 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    year={2018}
}&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Natural language generation (NLG) is a critical component in spoken dialogue systems.
Classic NLG can be divided into two phases: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sentence planning: deciding on the overall sentence structure, &lt;/li&gt;
&lt;li&gt;surface realization: determining specific word forms and flattening the sentence structure into a string. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Many simple NLG models are based on recurrent neural networks (RNN) and sequence-to-sequence (seq2seq) model, which basically contains a encoder-decoder structure; these NLG models generate sentences from scratch by jointly optimizing sentence planning and surface realization using a simple cross entropy loss training criterion.&lt;/p&gt;
&lt;p&gt;However, the simple encoder-decoder architecture usually suffers from generating complex and long sentences, because the decoder has to learn all grammar and diction knowledge.&lt;/p&gt;
&lt;p&gt;This paper introduces a hierarchical decoding NLG model based on linguistic patterns in different levels, and shows that the proposed method outperforms the traditional one with a smaller model size.
Furthermore, the design of the hierarchical decoding is flexible and easily-extendible in various NLG systems (The source code is available at &lt;a href=&quot;https://github.com/MiuLab/HNLG&quot;&gt;https://github.com/MiuLab/HNLG&lt;/a&gt;).&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Dynamic Time-Aware Attention to Speaker Roles and Contexts for Spoken Language Understanding]]></title><description><![CDATA[Shang-Yu Su*, Po-Chun Chen*, Ta-Chung Chi*, and Yun-Nung Chen (co-first author)]]></description><link>https://www.shangyusu.com/posts/asru2017/</link><guid isPermaLink="false">https://www.shangyusu.com/posts/asru2017/</guid><pubDate>Sat, 02 Sep 2017 22:40:32 GMT</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in Proceedings of 2017 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU 2017).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;https://arxiv.org/abs/1710.00165&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;,
&lt;a href=&quot;https://ieeexplore.ieee.org/document/8268985/&quot; target=&quot;_blank&quot;&gt;IEEE&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code:
&lt;a href=&quot;https://github.com/MiuLab/Time-SLU&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cite the paper:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;@inproceedings{chen2017dynamic,
  title={Dynamic Time-Aware Attention to Speaker Roles and Contexts for Spoken Language Understanding},
  author={Chen, Po-Chun and Chi, Ta-Chung and Su, Shang-Yu and Chen, Yun-Nung},
  booktitle={Proceedings of ASRU},
  year={2017}
}&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Spoken language understanding (SLU) is an essential component in conversational systems.
Most SLU components treat each utterance independently, and then the following components aggregate the multi-turn information in the separate phases.&lt;/p&gt;
&lt;p&gt;In order to avoid error propagation and effectively utilize contexts, prior works leveraged history for contextual SLU.
However, the previous models only paid attention to the content in history utterances without considering their temporal information and speaker roles.&lt;/p&gt;
&lt;p&gt;In dialogues, the most recent utterances should be more important than the least recent ones.
Furthermore, users usually pay attention to 1) self history for reasoning and 2) othersâ€™ utterances for listening, the speaker of the utterances may provides informative cues to help understanding.&lt;/p&gt;
&lt;p&gt;Therefore, this paper proposes an attention-based network that additionally leverages temporal information and speaker role for better SLU, where the attention to contexts and speaker roles can be automatically learned in an end-to-end manner.
The experiments on the benchmark Dialogue State Tracking Challenge 4 (DSTC4) dataset show that the time-aware dynamic role attention networks significantly improve the understanding performance (The released code: &lt;a href=&quot;https://github.com/MiuLab/Time-SLU&quot;&gt;https://github.com/MiuLab/Time-SLU&lt;/a&gt;).&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Speaker Role Contextual Modeling for Language Understanding and Dialogue Policy Learning]]></title><description><![CDATA[Shang-Yu Su*, Ta-Chung Chi*, Po-Chun Chen*, and Yun-Nung Chen (co-first author)]]></description><link>https://www.shangyusu.com/posts/humane-typography-in-the-digital-age/</link><guid isPermaLink="false">https://www.shangyusu.com/posts/humane-typography-in-the-digital-age/</guid><pubDate>Sat, 19 Aug 2017 22:40:32 GMT</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in Proceedings of The 8th International Joint Conference on Natural Language Processing (IJCNLP 2017).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;https://arxiv.org/abs/1710.00164&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;,
&lt;a href=&quot;https://aclweb.org/anthology/I17-2028&quot; target=&quot;_blank&quot;&gt;ACL&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code:
&lt;a href=&quot;https://github.com/MiuLab/Spk-Dialogue&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cite the paper:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;@inproceedings{chi2017speaker,
  author    = {Ta-Chung Chi and Po-Chun Chen and Shang-Yu Su and Yun-Nung Chen},
  title	    = {Speaker Role Contextual Modeling for Language Understanding and Dialogue Policy Learning},
  booktitle = {Proceedings of IJCNLP},
  year	    = {2017}
}&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Language understanding (LU) and dialogue policy learning are two essential components in conversational systems.
Human-human dialogues are not well-controlled and often random and unpredictable due to their own goals and speaking habits.&lt;/p&gt;
&lt;p&gt;This paper proposes a role-based contextual model to consider different speaker roles independently based on the various speaking patterns in the multi-turn dialogues.
The experiments on the benchmark dataset show that the proposed role-based model successfully learns role-specific behavioral patterns for contextual encoding and then significantly improves language understanding and dialogue policy learning tasks (The source code is available at: &lt;a href=&quot;https://github.com/MiuLab/Spk-Dialogue&quot;&gt;https://github.com/MiuLab/Spk-Dialogue&lt;/a&gt;).&lt;/p&gt;</content:encoded></item></channel></rss>