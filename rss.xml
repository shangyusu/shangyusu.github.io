<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Shang-Yu Su]]></title><description><![CDATA[Researcher of Natural Language Processing and Dialogue Systems.]]></description><link>https://www.shangyusu.com</link><generator>RSS for Node</generator><lastBuildDate>Thu, 10 Nov 2022 21:04:23 GMT</lastBuildDate><item><title><![CDATA[TREND: Trigger-Enhanced Relation-Extraction Network for Dialogues]]></title><description><![CDATA[Shang-Yu Su*, Po-Wei Lin*, Yun-Nung Chen]]></description><link>https://www.shangyusu.com/publications/sigdial2022-trend/</link><guid isPermaLink="false">https://www.shangyusu.com/publications/sigdial2022-trend/</guid><pubDate>Invalid Date</pubDate><content:encoded>&lt;p&gt;Resources:
&lt;a href=&quot;https://arxiv.org/abs/2108.13811&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The goal of dialogue relation extraction (DRE) is to identify the relation between two entities in a given dialogue. During conversations, speakers may expose their relations to certain entities by some clues, such evidences called “triggers”. However, none of the existing work on DRE tried to detect triggers and leverage the information for enhancing the performance. This paper proposes TREND, a multi-tasking BERT-based model which learns to identify triggers for improving relation extraction. The experimental results show that the proposed method achieves the state-of-the-art on the benchmark datasets.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[BARCOR: Towards A Unified Framework for Conversational Recommendation Systems]]></title><description><![CDATA[Ting-Chun Wang, Shang-Yu Su, Yun-Nung Chen]]></description><link>https://www.shangyusu.com/publications/barcor/</link><guid isPermaLink="false">https://www.shangyusu.com/publications/barcor/</guid><pubDate>Invalid Date</pubDate><content:encoded>&lt;p&gt;Resources:
&lt;a href=&quot;https://arxiv.org/abs/2203.14257&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Recommendation systems focus on helping users find items of interest in the situations of information overload, where users’ preferences are typically estimated by the past observed behaviors. In contrast, conversational recommendation systems (CRS) aim to understand users’ preferences via interactions in conversation flows. CRS is a complex problem that consists of two main tasks: (1) recommendation and (2) response generation. Previous work often tried to solve the problem in a modular manner, where recommenders and response generators are separate neural models. Such modular architectures often come with a complicated and unintuitive connection between the modules, leading to inefficient learning and other issues. In this work, we propose a unified framework based on BART for conversational recommendation, which tackles two tasks in a single model. Furthermore, we also design and collect a lightweight knowledge graph for CRS in the movie domain. The experimental results show that the proposed methods achieve the state-of-the-art performance in terms of both automatic and human evaluation.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[HUMBO: Bridging Response Generation and Facial Expression Synthesis]]></title><description><![CDATA[Shang-Yu Su*, Po-Wei Lin*, Yun-Nung Chen]]></description><link>https://www.shangyusu.com/publications/humbo/</link><guid isPermaLink="false">https://www.shangyusu.com/publications/humbo/</guid><pubDate>Invalid Date</pubDate><content:encoded>&lt;p&gt;Resources:
&lt;a href=&quot;https://arxiv.org/abs/1905.11240&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Spoken dialogue systems that assist users to solve complex tasks such as movie ticket booking have become an emerging research topic in artificial intelligence and natural language processing areas. With a well-designed dialogue system as an intelligent personal assistant, people can accomplish certain tasks more easily via natural language interactions. Today there are several virtual intelligent assistants in the market; however, most systems only focus on textual or vocal interaction. In this paper, we present HUMBO, a system aiming at generating dialogue responses and simultaneously synthesize corresponding visual expressions on faces for better multimodal interaction. HUMBO can (1) let users determine the appearances of virtual assistants by a single image, and (2) generate coherent emotional utterances and facial expressions on the user-provided image. This is not only a brand new research direction but more importantly, an ultimate step toward more human-like virtual assistants.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Exploiting the Duality between Language Understanding and Generation and Beyond]]></title><description><![CDATA[Shang-Yu Su]]></description><link>https://www.shangyusu.com/publications/phd-thesis/</link><guid isPermaLink="false">https://www.shangyusu.com/publications/phd-thesis/</guid><pubDate>Tue, 19 Oct 2021 22:40:32 GMT</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This thesis was recognized by &lt;a href=&quot;https://www.aclclp.org.tw/grants_c.php&quot; target=&quot;_blank&quot;&gt;ACLCLP Best Thesis Award (2022)&lt;/a&gt;.&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Resources:
&lt;a href=&quot;/PhD_Thesis_Shang_Yu_Su_cover-b1275b0ca06ec8e1ed840c18c7776f6c.pdf&quot; target=&quot;_blank&quot;&gt;full thesis&lt;/a&gt;,
&lt;a href=&quot;https://etds.ncl.edu.tw/cgi-bin/gs32/gsweb.cgi/ccd=6nzfZD/record?r1=1&amp;h1=1&quot; target=&quot;_blank&quot;&gt;NDLTD&lt;/a&gt;,
&lt;a href=&quot;/defense_v1.0-002974f812bdd246cf6abe35d0efdf25.pdf&quot; target=&quot;_blank&quot;&gt;slides&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Many real­world artificial intelligence tasks come with a dual form; that is, we could
directly swap the input and the target of a task to formulate another task. Machine translation is a classic example, for example, translating from English to Chinese has a dual task
of translating from Chinese to English. Automatic speech recognition (ASR) and text­-to-speech (TTS) also have structural duality. Given a piece of informative context, question
answering and question generation are in dual form. The recent studies magnified the importance of the duality by boosting the performance of both tasks with the exploitation of the duality.&lt;/p&gt;
&lt;p&gt;Natural language understanding (NLU) and natural language generation (NLG) are
both critical research topics in the NLP and dialogue fields. The goal of natural language understanding is to extract the core semantic meaning from the given utterances, while
natural language generation is opposite, of which the goal is to construct corresponding
sentences based on the given semantics. However, the dual property between understanding and generation has been rarely explored.&lt;/p&gt;
&lt;p&gt;This main goal of this dissertation is to investigate the structural duality between NLU
and NLG. In this thesis, we present five consecutive studies, each focuses on different
aspects of learning and data settings. First, we exploits the duality between NLU and
NLG and introduces it into the learning objective as the regularization term. Moreover,
expert knowledge is incorporated to design suitable approaches for estimating data distribution. Second, we further propose a joint learning framework, which provides flexibility of incorporating not only supervised but also unsupervised learning algorithms and enables the gradient to propagate through two modules seamlessly. Third, we study how to
enhance the joint framework by mutual information maximization. Fourth, since above
works exploit the duality in the training stage, hence we make a step forward to leverage
the duality in the inference stage. Lastly, we finetune the pretrained language models on
the two dual tasks and achieve the goal of solving two dual tasks in a single model. Each
work presents a new model and learning framework exploiting the duality in different
manners. Together, this dissertation explores a new research direction of exploiting the
duality between language understanding and generation.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Lifelong Language Knowledge Distillation]]></title><description><![CDATA[Yung-Sung Chuang, Shang-Yu Su, and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/publications/emnlp2020-l2kd/</link><guid isPermaLink="false">https://www.shangyusu.com/publications/emnlp2020-l2kd/</guid><pubDate>Sat, 19 Sep 2020 22:40:32 GMT</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper will be published in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020), Virtual, November 16-20, 2020. ACL.&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;https://arxiv.org/abs/2010.02123&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It is challenging to perform lifelong language learning (LLL) on a stream of different tasks without any performance degradation comparing to the multi-task counterparts. To address this issue, we present Lifelong Language Knowledge Distillation (L2KD), a simple but efficient method that can be easily applied to existing LLL architectures in order to mitigate the degradation. Specifically, when the LLL model is trained on a new task, we assign a teacher model to first learn the new task, and pass the knowledge to the LLL model via knowledge distillation. Therefore, the LLL model can better adapt to the new task while keeping the previously learned knowledge. Experiments show that the proposed L2KD consistently improves previous state-of-the-art models, and the degradation comparing to multi-task models in LLL tasks is well mitigated for both sequence generation and text classification tasks.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Dual Inference for Improving Language Understanding and Generation]]></title><description><![CDATA[Shang-Yu Su, Yung-Sung Chuang, and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/publications/emnlp2020-dual-inf/</link><guid isPermaLink="false">https://www.shangyusu.com/publications/emnlp2020-dual-inf/</guid><pubDate>Fri, 18 Sep 2020 22:40:32 GMT</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper will be published in Findings of EMNLP, Virtual, November 16-20, 2020. ACL.&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;https://arxiv.org/abs/2010.04246&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Natural language understanding (NLU) and Natural language generation (NLG) tasks hold a strong dual relationship, where NLU aims at predicting semantic labels based on natural language utterances and NLG does the opposite.
The prior work mainly focused on exploiting the duality in model training in order to obtain the models with better performance.
However, regarding the fast-growing scale of models in the current NLP area, sometimes we may have difficulty retraining whole NLU and NLG models.
To better address the issue, this paper proposes to leverage the duality in the inference stage without the need of retraining.
The experiments on three benchmark datasets demonstrate the effectiveness of the proposed method in both NLU and NLG, providing the great potential of practical usage.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Towards Unsupervised Language Understanding and Generation by Joint Dual Learning]]></title><description><![CDATA[Shang-Yu Su, Chao-Wei Huang, and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/publications/acl2020-jdl/</link><guid isPermaLink="false">https://www.shangyusu.com/publications/acl2020-jdl/</guid><pubDate>Fri, 03 Apr 2020 22:40:32 GMT</pubDate><content:encoded>&lt;p&gt;Full paper:
&lt;a href=&quot;/acl2020-b9252510ccde72c8db3c727a50032442.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;,
&lt;a href=&quot;https://arxiv.org/abs/2004.14710&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;This paper is published in Proceedings of The 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;In modular dialogue systems, natural language understanding (NLU) and natural language generation (NLG) are two critical components, where NLU extracts the semantics from the given texts and NLG is to construct corresponding natural language sentences based on the input semantic representations.
However, the dual property between understanding and generation has been rarely explored.
The prior work is the first attempt that utilized the duality between NLU and NLG to improve the performance via a dual supervised learning framework.
However, the prior work still learned both components in &lt;em&gt;supervised&lt;/em&gt; manner, instead, this paper introduces a general learning framework to effectively exploit such duality, providing flexibility of incorporating both &lt;em&gt;supervised&lt;/em&gt; and &lt;em&gt;unsupervised&lt;/em&gt; learning algorithms to train language understanding and generation models in a joint fashion.
The benchmark experiments demonstrate that the proposed approach is capable of boosting the performance of both NLU and NLG.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Creative AI (A Section of "Introduction to Artificial Intelligence")]]></title><description><![CDATA[Shang-Yu Su and Hung-Yi Lee]]></description><link>https://www.shangyusu.com/publications/book-ai/</link><guid isPermaLink="false">https://www.shangyusu.com/publications/book-ai/</guid><pubDate>Tue, 08 Oct 2019 22:40:32 GMT</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;“Introduction to Artificial Intelligence” is an AI textbook written in Chinese for high school students.&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.books.com.tw/products/0010826415&quot; target=&quot;_blank&quot;&gt;online store&lt;/a&gt;
&lt;a href=&quot;https://www.facebook.com/foxconnai/?ref=py_c&quot; target=&quot;_blank&quot;&gt;FB page&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Creative AI is the section about Generative Adversarial Networks, including introduction to basic concepts of adversarial learning, vanilla GAN, conditional GAN, and CycleGAN.&lt;/p&gt;
&lt;p&gt;
  &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/book-8a8adad94b8000d0a88260ca097b8e74-d7b6d.jpg&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
  
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; ; max-width: 566px; margin-left: auto; margin-right: auto;&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 141.3427561837456%; position: relative; bottom: 0; left: 0; background-image: url(&amp;apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAcABQDASIAAhEBAxEB/8QAGAABAQEBAQAAAAAAAAAAAAAAAAMEAQL/xAAYAQACAwAAAAAAAAAAAAAAAAACBAABA//aAAwDAQACEAMQAAAB3z0cyMAPPG/hlayaX//EABsQAAIDAAMAAAAAAAAAAAAAAAECABASESEi/9oACAEBAAEFAixm3nNdGnYwN71S1//EABgRAAIDAAAAAAAAAAAAAAAAAAABEBES/9oACAEDAQE/AXFs0z//xAAXEQADAQAAAAAAAAAAAAAAAAAAARIQ/9oACAECAQE/AVkohH//xAAcEAABBAMBAAAAAAAAAAAAAAAAARARMSAhIjL/2gAIAQEABj8Co8vpWiBOSsP/xAAcEAACAgIDAAAAAAAAAAAAAAABEQBRECExYXH/2gAIAQEAAT8hbK8YANbifUDuQ1DCxpVuNoQmaYJnH//aAAwDAQACAAMAAAAQYA48/8QAGREAAwADAAAAAAAAAAAAAAAAAAERMUFh/9oACAEDAQE/EMpRJQ2U7n//xAAZEQADAAMAAAAAAAAAAAAAAAAAAREQIWH/2gAIAQIBAT8QbVg27j4n/8QAHhABAQACAgIDAAAAAAAAAAAAAREAITFBUXFhgZH/2gAIAQEAAT8QkVggot85NZ2nDvEaN4kVQAt8BzjBCV6M84K7I+8JUsRCH194ybDlbv8AHHLHwuOxuVRA10Y5/9k=&amp;apos;); background-size: cover; display: block;&quot;&gt;
      &lt;img class=&quot;gatsby-resp-image-image&quot; style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot; alt=&quot;book&quot; title=&quot;&quot; src=&quot;/static/book-8a8adad94b8000d0a88260ca097b8e74-d7b6d.jpg&quot; srcset=&quot;/static/book-8a8adad94b8000d0a88260ca097b8e74-d683c.jpg 240w,
/static/book-8a8adad94b8000d0a88260ca097b8e74-8b24b.jpg 480w,
/static/book-8a8adad94b8000d0a88260ca097b8e74-d7b6d.jpg 566w&quot; sizes=&quot;(max-width: 566px) 100vw, 566px&quot;&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    &lt;/img&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Dual Supervised Learning for Natural Language Understanding and Generation]]></title><description><![CDATA[Shang-Yu Su, Chao-Wei Huang, and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/publications/acl2019-dsl/</link><guid isPermaLink="false">https://www.shangyusu.com/publications/acl2019-dsl/</guid><pubDate>Fri, 17 May 2019 22:40:32 GMT</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in Proceedings of The 57th Annual Meeting of the Association for Computational Linguistics (ACL2019).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/acl20192-40700108315661f3bdcf6771cfada835.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;,
&lt;a href=&quot;https://arxiv.org/abs/1905.06196&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Natural language understanding (NLU) and natural language generation (NLG) are both critical research topics in the NLP field.
Natural language understanding is to extract the core semantic meaning from the given utterances, while natural language generation is opposite, of which the goal is to construct corresponding sentences based on the given semantics.
However, such dual relationship has not been investigated in the literature.
This paper proposes a new learning framework for language understanding and generation on top of dual supervised learning, providing a way to exploit the duality.
The preliminary experiments show that the proposed approach boosts the performance for both tasks. &lt;/p&gt;</content:encoded></item><item><title><![CDATA[Modeling Melodic Feature Dependency with Modularized Variational Auto-Encoder]]></title><description><![CDATA[Yu-An Wang*, Yu-Kai Huang*, Tzu-Chuan Lin*, Shang-Yu Su, and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/publications/icassp2019-music/</link><guid isPermaLink="false">https://www.shangyusu.com/publications/icassp2019-music/</guid><pubDate>Invalid Date</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in the in Proceedings of The 44th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2019), Brighton, U.K., May 12-17, 2019. IEEE.&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/ICASSP_2019___Music_Generation-81c090fb9606994fe2125873e57dbc71.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;,
&lt;a href=&quot;https://arxiv.org/abs/1811.00162&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Automatic melody generation has been a long-time aspiration for both AI researchers and musicians.
However, learning to generate euphonious melodies has turned out to be highly challenging.
This paper introduces 1) a new variant of variational autoencoder (VAE), where the model structure is designed in a modularized manner in order to model polyphonic and dynamic music with domain knowledge, and 2) a hierarchical encoding/decoding strategy, which explicitly models the dependency between melodic features. The proposed framework is capable of generating distinct melodies that sounds natural, and the experiments for evaluating generated music clips show that the proposed model outperforms the baselines in human evaluation. (The first three authors contributed equally.)&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Compound Variational Auto-Encoder]]></title><description><![CDATA[Shang-Yu Su, Shan-Wei Lin, and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/publications/icassp2019-comvae/</link><guid isPermaLink="false">https://www.shangyusu.com/publications/icassp2019-comvae/</guid><pubDate>Invalid Date</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in the in Proceedings of The 44th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2019), Brighton, U.K., May 12-17, 2019. IEEE.&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/ICASSP_ComVAE-34ac75152d91d929def8cf5c13d83b62.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Amortized variational inference (AVI) enables efficient training of deep generative models to scale to large datasets. The quality of the approximate inference is determined by various reasons, such as the ability of producing proper variational parameters for each datapoint in the recognition network and whether the variational distribution matches the true posterior, etc.
This paper focuses on the inference sub-optimality of variational auto-encoders (VAE), where the goal is to reduce the difference caused by amortizing the variational distribution parameters over the entire training set instead of optimizing for each training example individually, which is also known as the amortization gap.
This paper extends Bayesian inference in VAE from the latent level to both latent and weight levels by adopting Bayesian neural networks (BNN) in the encoder, so that each datapoint obtains its own distribution for better modeling.
The hybrid design in the proposed compound VAE is empirically demonstrated to be capable of mitigating the amortization gap.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Dynamically Context-Sensitive Time-Decay Attention for Dialogue Modeling]]></title><description><![CDATA[Shang-Yu Su, Pei-Chieh Yuan, and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/publications/icassp2019-cnlu/</link><guid isPermaLink="false">https://www.shangyusu.com/publications/icassp2019-cnlu/</guid><pubDate>Invalid Date</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in the in Proceedings of The 44th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2019), Brighton, U.K., May 12-17, 2019. IEEE.&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/ICASSP_2019___Context_Sensitive_Time_Decay-0592df6fa69ea234e824ec9a1ddf928c.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;,
&lt;a href=&quot;https://arxiv.org/abs/1809.01557&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Spoken language understanding (SLU) is an essential component in conversational systems. Considering that contexts provide informative cues for better understanding, history can be leveraged for contextual SLU. However, most prior work only paid attention to the related content in history utterances and ignored the temporal information. In dialogues, it is intuitive that the most recent utterances are more important than the least recent ones, hence time-aware attention should be in a decaying manner. Therefore, this paper allows the model to automatically learn a time-decay attention function based on the content of each role’s contexts, which effectively integrates both content-aware and time-aware perspectives and demonstrates remarkable flexibility to complex dialogue contexts. The experiments on the benchmark Dialogue State Tracking Challenge (DSTC4) dataset show that the proposed role-based context-sensitive time-decay attention mechanisms significantly improve the state-of-the-art model for contextual understanding performance. &lt;/p&gt;</content:encoded></item><item><title><![CDATA[Reactive Multi-Stage Feature Fusion for Multimodal Dialogue Modeling]]></title><description><![CDATA[Yi-Ting Yeh, Tzu-Chuan Lin, Hsiao-Hua Cheng, Yi-Hsuan Deng, Shang-Yu Su, and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/publications/aaai2019-dstc7-avsd/</link><guid isPermaLink="false">https://www.shangyusu.com/publications/aaai2019-dstc7-avsd/</guid><pubDate>Invalid Date</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in the 7th Dialog System Technology Challenge (DSTC7) in the Proceedings of Thirty-Third AAAI Conference on Artificial Intelligence (AAAI 2019 - DSTC7).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/AAAI_2019___DSTC_Track_3-734c834eba9b64f63af0fae9f9030aba.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Visual question answering and visual dialogue tasks have been increasingly studied in the multimodal field towards more practical real-world scenarios.
A more challenging task, audio visual scene-aware dialogue (AVSD), is proposed to further advance the technologies that connect audio, vision, and language, which introduces temporal video information and dialogue interactions between a questioner and an answerer.
This paper proposes an intuitive mechanism that fuses features and attention in multiple stages in order to well integrate multimodal features, and the results demonstrate its capability in the experiments.
Also, we apply several state-of-the-art models in other tasks to the AVSD task, and further analyze their generalization across different tasks.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Knowledge-Grounded Response Generation with Deep Attentional Latent-Variable Model]]></title><description><![CDATA[Hao-Tong Ye, Kai-Ling Lo, Shang-Yu Su, and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/publications/aaai2019-dstc7-cvae/</link><guid isPermaLink="false">https://www.shangyusu.com/publications/aaai2019-dstc7-cvae/</guid><pubDate>Invalid Date</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in the 7th Dialog System Technology Challenge (DSTC7) in the Proceedings of Thirty-Third AAAI Conference on Artificial Intelligence (AAAI 2019 - DSTC7).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;NOTE: Also published in Computer Speech and Language (Journal, 2020)&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/AAAI_2019___DSTC_Track_2-1ac5ec1418e3e3049e3e156b0d215230.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;End-to-end dialogue generation has achieved promising results without using handcrafted features and attributes specific for each task and corpus. However, one of the fatal drawbacks in such approaches is that they are unable to generate informative utterances, so it limits their usage from some real-world conversational applications. This paper attempts at generating diverse and informative responses with a variational generation model, which contains a joint attention mechanism conditioning on the information from both dialogue contexts and extra knowledge.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Learning Multi-Level Information for Dialogue Response Selection by Highway Recurrent Transformer]]></title><description><![CDATA[Ting-Rui Chiang, Chao-Wei Huang, Shang-Yu Su, and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/publications/aaai2019-dstc7-hrt/</link><guid isPermaLink="false">https://www.shangyusu.com/publications/aaai2019-dstc7-hrt/</guid><pubDate>Invalid Date</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in the 7th Dialog System Technology Challenge (DSTC7) in the Proceedings of Thirty-Third AAAI Conference on Artificial Intelligence (AAAI 2019 - DSTC7).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;NOTE: Also published in Computer Speech and Language (Journal, 2020)&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/AAAI_2019___DSTC_Track_1_2-1c0c2c056933540e847486427a9bfc64.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;With the increasing research interest in dialogue response generation, there is an emerging branch formulating this task as selecting next sentences, where given the partial dialogue contexts, the goal is to determine the most probable next sentence.
Following the recent success of the Transformer model, this paper proposes (1) a new variant of attention mechanism based on multi-head attention, called highway attention, and (2) a recurrent model based on transformer and the proposed highway attention, so-called Highway Recurrent Transformer.
Experiments on the response selection task in the seventh Dialog System Technology Challenge (DSTC7) show the capability of the proposed model of modeling both utterance-level and dialogue-level information; the effectiveness of each module is further analyzed as well.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[RAP-Net: Recurrent Attention Pooling Networks for Dialogue Response Selection]]></title><description><![CDATA[Chao-Wei Huang, Ting-Rui Chiang, Shang-Yu Su, and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/publications/aaai2019-dstc7-rapnet/</link><guid isPermaLink="false">https://www.shangyusu.com/publications/aaai2019-dstc7-rapnet/</guid><pubDate>Invalid Date</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in the 7th Dialog System Technology Challenge (DSTC7) in the Proceedings of Thirty-Third AAAI Conference on Artificial Intelligence (AAAI 2019 - DSTC7).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;NOTE: Also published in Computer Speech and Language (Journal, 2020)&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/AAAI_2019___DSTC_Track_1-2d7495af163e0096f9714141fdcdc7aa.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The response selection has been an emerging research topic due to the growing interest in dialogue modeling, where the goal of the task is to select an appropriate response for continuing dialogues.
To further push the end-to-end dialogue model toward real-world scenarios, the seventh Dialog System Technology Challenge (DSTC7) proposed a challenge track based on real chatlog datasets.
The competition focuses on dialogue modeling with several advanced characteristics: (1) natural language diversity, (2) capability of precisely selecting a proper response from a large set of candidates or the scenario without any correct answer, and (3) knowledge grounding.
This paper introduces recurrent attention pooling networks (RAP-Net), a novel framework for response selection, which can well estimate the relevance between the dialogue contexts and the candidates.
The proposed RAP-Net is shown to be effective and can be generalize across different datasets and settings in the DSTC7 experiments.
In the future, the proposed model can be evaluated on other retrieval-based tasks to test the model capability of generalization.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Investigating Linguistic Pattern Ordering in Hierarchical Natural Language Generation]]></title><description><![CDATA[Shang-Yu Su, and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/publications/slt2018-hnlg/</link><guid isPermaLink="false">https://www.shangyusu.com/publications/slt2018-hnlg/</guid><pubDate>Invalid Date</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in Proceedings of 7th IEEE Workshop on Spoken Language Technology (SLT 2018).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/slt-2018-hnlg-bd09cbcfc6315cef7fbf9653703534f9.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;,
&lt;a href=&quot;https://arxiv.org/abs/1809.07629&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code:
&lt;a href=&quot;https://github.com/MiuLab/HNLG&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Natural language generation (NLG) is a critical component in spoken dialogue system, which can be divided into two phases: (1) sentence planning: deciding the overall sentence structure, (2) surface realization: determining specific word forms and flattening the sentence structure into a string.
With the rise of deep learning, most modern NLG models are based on a sequence-to-sequence (seq2seq) model, which basically contains an encoder-decoder structure; these NLG models generate sentences from scratch by jointly optimizing sentence planning and surface realization.
However, such simple encoder-decoder architecture usually fail to generate complex and long sentences, because the decoder has difficulty learning all grammar and diction knowledge well.
This paper introduces an NLG model with a hierarchical attentional decoder, where the hierarchy focuses on leveraging linguistic knowledge in a specific order. The experiments show that the proposed method significantly outperforms the traditional seq2seq model with a smaller model size, and the design of the hierarchical attentional decoder can be applied to various NLG systems.
Furthermore, different generation strategies based on linguistic patterns are investigated and analyzed in order to guide future NLG research work.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Discriminative Deep Dyna-Q: Robust Planning for Dialogue Policy Learning]]></title><description><![CDATA[Shang-Yu Su, Xiujun Li, Jianfeng Gao, Jingjing Liu and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/publications/emnlp2018-d3q/</link><guid isPermaLink="false">https://www.shangyusu.com/publications/emnlp2018-d3q/</guid><pubDate>Mon, 13 Aug 2018 22:40:32 GMT</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP 2018).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/emnlp-2018-d3q-b71cd121612e0c757f51ccd99c6b5480.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;,
&lt;a href=&quot;https://arxiv.org/abs/1808.09442&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This paper presents a Discriminative Deep Dyna-Q (D3Q) approach to improving the effectiveness and robustness of DDQ, a recently proposed framework that extends the Dyna-Q algorithm to integrate planning for task-completion dialogue policy learning. To obviate DDQ’s high dependency on the quality of simulated experiences, we incorporate an RNN-based discriminator in D3Q to differentiate simulated experience from real user experience in order to control the quality of training data. Experiments show that D3Q significantly outperforms DDQ by controlling the quality of simulated experience used for planning. The effectiveness and robustness of D3Q is further demonstrated in a domain extension setting, where the agent’s capability of adapting to a changing environment is tested.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning]]></title><description><![CDATA[Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu, Kam-Fai Wong, and Shang-Yu Su]]></description><link>https://www.shangyusu.com/publications/acl2018-ddq/</link><guid isPermaLink="false">https://www.shangyusu.com/publications/acl2018-ddq/</guid><pubDate>Sat, 10 Feb 2018 22:40:32 GMT</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in Proceedings of The 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/acl-2018-deep-4efbe3c99de146207ef31cd7523496ed.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;,
&lt;a href=&quot;https://arxiv.org/abs/1801.06176&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code:
&lt;a href=&quot;https://github.com/MiuLab/DDQ&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cite the paper:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;@article{peng2018integrating,
  title={Integrating planning for task-completion dialogue policy learning},
  author={Peng, Baolin and Li, Xiujun and Gao, Jianfeng and Liu, Jingjing and Wong, Kam-Fai and Su, Shang-Yu},
  journal={arXiv preprint arXiv:1801.06176},
  year={2018}
}&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Training a task-completion dialogue agent via reinforcement learning (RL) is costly because it requires many interactions with real users. One common alternative is to use a user simulator. However, a user simulator usually lacks the language complexity of human interlocutors and the biases in its design may tend to degrade the agent. To address these issues, we present Deep Dyna-Q, which to our knowledge is the first deep RL framework that integrates planning for task-completion dialogue policy learning. We incorporate into the dialogue agent a model of the environment, referred to as the world model, to mimic real user response and generate simulated experience. During dialogue policy learning, the world model is constantly updated with real user experience to approach real user behavior, and in turn, the dialogue agent is optimized using both real experience and simulated experience. The effectiveness of our approach is demonstrated on a movie-ticket booking task in both simulated and human-in-the-loop settings.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[How Time Matters: Learning Time-Decay Attention for Contextual Spoken Language Understanding in Dialogues]]></title><description><![CDATA[Shang-Yu Su, Pei-Chieh Yuan, and Yun-Nung Chen]]></description><link>https://www.shangyusu.com/publications/naacl2018-nlu/</link><guid isPermaLink="false">https://www.shangyusu.com/publications/naacl2018-nlu/</guid><pubDate>Sat, 03 Feb 2018 22:40:32 GMT</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in Proceedings of The 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2018).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/naacl-hlt-2018-nlu-camera-ready-17349e002beae6efda18798e5d19707d.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code:
&lt;a href=&quot;https://github.com/MiuLab/Time-Decay-SLU&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cite the paper:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;@inproceedings{su2018how,
  title={How time matters: Learning Time-Decay Attention for Contextual Spoken Language Understanding in Dialogues},
    author={Shang-Yu Su, Pei-Chieh Yuan, and Yun-Nung Chen},
    booktitle={Proceedings of The 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    year={2018}
}&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Spoken language understanding (SLU) is an essential component in conversational systems. Most SLU components treats each utterance independently, and then the following components aggregate the multi-turn information in the separate phases.
In order to avoid error propagation and effectively utilize contexts, prior work leveraged history for contextual SLU.&lt;/p&gt;
&lt;p&gt;However, most previous models only paid attention to the related content in history utterances, ignoring their temporal information.
In the dialogues, it is intuitive that the most recent utterances are more important than the least recent ones, in other words, time-aware attention should be in a decaying manner.&lt;/p&gt;
&lt;p&gt;Therefore, this paper designs and investigates various types of time-decay attention on the sentence-level and speaker-level, and further proposes a flexible universal time-decay attention mechanism.
The experiments on the benchmark Dialogue State Tracking Challenge (DSTC4) dataset show that the proposed time-decay attention mechanisms significantly improve the state-of-the-art model for contextual understanding performance (The source code is at: &lt;a href=&quot;https://github.com/MiuLab/Time-Decay-SLU&quot;&gt;https://github.com/MiuLab/Time-Decay-SLU&lt;/a&gt;).&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Natural Language Generation by Hierarchical Decoding with Linguistic Patterns]]></title><description><![CDATA[Shang-Yu Su*, Kai-Ling Lo*, Yi-Ting Yeh, and Yun-Nung Chen (co-first author)]]></description><link>https://www.shangyusu.com/publications/naacl2018-nlg/</link><guid isPermaLink="false">https://www.shangyusu.com/publications/naacl2018-nlg/</guid><pubDate>Fri, 02 Feb 2018 22:40:32 GMT</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in Proceedings of The 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2018).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;/naacl-hlt-2018-nlg-1a01ffc4d8392caed4366ed051fc5565.pdf&quot; target=&quot;_blank&quot;&gt;Here&lt;/a&gt;,
&lt;a href=&quot;https://arxiv.org/abs/1808.02747&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code:
&lt;a href=&quot;https://github.com/MiuLab/HNLG&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cite the paper:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;@inproceedings{su2018natural,
  title={Natural Language Generation by Hierarchical Decoding with Linguistic Patterns},
    author={Shang-Yu Su, Kai-Ling Lo, Yi-Ting Yeh, and Yun-Nung Chen},
    booktitle={Proceedings of The 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    year={2018}
}&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Natural language generation (NLG) is a critical component in spoken dialogue systems.
Classic NLG can be divided into two phases: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sentence planning: deciding on the overall sentence structure, &lt;/li&gt;
&lt;li&gt;surface realization: determining specific word forms and flattening the sentence structure into a string. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Many simple NLG models are based on recurrent neural networks (RNN) and sequence-to-sequence (seq2seq) model, which basically contains a encoder-decoder structure; these NLG models generate sentences from scratch by jointly optimizing sentence planning and surface realization using a simple cross entropy loss training criterion.&lt;/p&gt;
&lt;p&gt;However, the simple encoder-decoder architecture usually suffers from generating complex and long sentences, because the decoder has to learn all grammar and diction knowledge.&lt;/p&gt;
&lt;p&gt;This paper introduces a hierarchical decoding NLG model based on linguistic patterns in different levels, and shows that the proposed method outperforms the traditional one with a smaller model size.
Furthermore, the design of the hierarchical decoding is flexible and easily-extendible in various NLG systems (The source code is available at &lt;a href=&quot;https://github.com/MiuLab/HNLG&quot;&gt;https://github.com/MiuLab/HNLG&lt;/a&gt;).&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Dynamic Time-Aware Attention to Speaker Roles and Contexts for Spoken Language Understanding]]></title><description><![CDATA[Shang-Yu Su*, Po-Chun Chen*, Ta-Chung Chi*, and Yun-Nung Chen (co-first author)]]></description><link>https://www.shangyusu.com/publications/asru2017/</link><guid isPermaLink="false">https://www.shangyusu.com/publications/asru2017/</guid><pubDate>Sat, 02 Sep 2017 22:40:32 GMT</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in Proceedings of 2017 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU 2017).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;https://arxiv.org/abs/1710.00165&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;,
&lt;a href=&quot;https://ieeexplore.ieee.org/document/8268985/&quot; target=&quot;_blank&quot;&gt;IEEE&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code:
&lt;a href=&quot;https://github.com/MiuLab/Time-SLU&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cite the paper:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;@inproceedings{chen2017dynamic,
  title={Dynamic Time-Aware Attention to Speaker Roles and Contexts for Spoken Language Understanding},
  author={Chen, Po-Chun and Chi, Ta-Chung and Su, Shang-Yu and Chen, Yun-Nung},
  booktitle={Proceedings of ASRU},
  year={2017}
}&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Spoken language understanding (SLU) is an essential component in conversational systems.
Most SLU components treat each utterance independently, and then the following components aggregate the multi-turn information in the separate phases.&lt;/p&gt;
&lt;p&gt;In order to avoid error propagation and effectively utilize contexts, prior works leveraged history for contextual SLU.
However, the previous models only paid attention to the content in history utterances without considering their temporal information and speaker roles.&lt;/p&gt;
&lt;p&gt;In dialogues, the most recent utterances should be more important than the least recent ones.
Furthermore, users usually pay attention to 1) self history for reasoning and 2) others’ utterances for listening, the speaker of the utterances may provides informative cues to help understanding.&lt;/p&gt;
&lt;p&gt;Therefore, this paper proposes an attention-based network that additionally leverages temporal information and speaker role for better SLU, where the attention to contexts and speaker roles can be automatically learned in an end-to-end manner.
The experiments on the benchmark Dialogue State Tracking Challenge 4 (DSTC4) dataset show that the time-aware dynamic role attention networks significantly improve the understanding performance (The released code: &lt;a href=&quot;https://github.com/MiuLab/Time-SLU&quot;&gt;https://github.com/MiuLab/Time-SLU&lt;/a&gt;).&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Speaker Role Contextual Modeling for Language Understanding and Dialogue Policy Learning]]></title><description><![CDATA[Shang-Yu Su*, Ta-Chung Chi*, Po-Chun Chen*, and Yun-Nung Chen (co-first author)]]></description><link>https://www.shangyusu.com/publications/humane-typography-in-the-digital-age/</link><guid isPermaLink="false">https://www.shangyusu.com/publications/humane-typography-in-the-digital-age/</guid><pubDate>Sat, 19 Aug 2017 22:40:32 GMT</pubDate><content:encoded>&lt;p&gt;&lt;b&gt;This paper is published in Proceedings of The 8th International Joint Conference on Natural Language Processing (IJCNLP 2017).&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Full paper:
&lt;a href=&quot;https://arxiv.org/abs/1710.00164&quot; target=&quot;_blank&quot;&gt;arXiv&lt;/a&gt;,
&lt;a href=&quot;https://aclweb.org/anthology/I17-2028&quot; target=&quot;_blank&quot;&gt;ACL&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code:
&lt;a href=&quot;https://github.com/MiuLab/Spk-Dialogue&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cite the paper:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;@inproceedings{chi2017speaker,
  author    = {Ta-Chung Chi and Po-Chun Chen and Shang-Yu Su and Yun-Nung Chen},
  title	    = {Speaker Role Contextual Modeling for Language Understanding and Dialogue Policy Learning},
  booktitle = {Proceedings of IJCNLP},
  year	    = {2017}
}&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Language understanding (LU) and dialogue policy learning are two essential components in conversational systems.
Human-human dialogues are not well-controlled and often random and unpredictable due to their own goals and speaking habits.&lt;/p&gt;
&lt;p&gt;This paper proposes a role-based contextual model to consider different speaker roles independently based on the various speaking patterns in the multi-turn dialogues.
The experiments on the benchmark dataset show that the proposed role-based model successfully learns role-specific behavioral patterns for contextual encoding and then significantly improves language understanding and dialogue policy learning tasks (The source code is available at: &lt;a href=&quot;https://github.com/MiuLab/Spk-Dialogue&quot;&gt;https://github.com/MiuLab/Spk-Dialogue&lt;/a&gt;).&lt;/p&gt;</content:encoded></item></channel></rss>