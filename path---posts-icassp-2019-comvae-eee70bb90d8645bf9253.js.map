{"version":3,"sources":["webpack:///path---posts-icassp-2019-comvae-eee70bb90d8645bf9253.js","webpack:///./.cache/json/posts-icassp-2019-comvae.json"],"names":["webpackJsonp","428","module","exports","data","site","siteMetadata","title","subtitle","copyright","author","name","twitter","disqusShortname","url","markdownRemark","id","html","fields","tagSlugs","frontmatter","tags","date","description","conference","pathContext","slug"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,MAAQC,cAAgBC,MAAA,cAAAC,SAAA,wIAAAC,UAAA,yBAAAC,QAAwNC,KAAA,cAAAC,QAAA,KAAmCC,gBAAA,GAAAC,IAAA,8BAAyDC,gBAAmBC,GAAA,qIAAAC,KAAA,6pCAAAC,QAA4zCC,UAAA,eAA0BC,aAAgBb,MAAA,oCAAAc,MAAA,OAAAC,KAAA,0BAAAC,YAAA,+CAAAC,WAAA,gBAAqLC,aAAgBC,KAAA","file":"path---posts-icassp-2019-comvae-eee70bb90d8645bf9253.js","sourcesContent":["webpackJsonp([228534894703402],{\n\n/***/ 428:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"site\":{\"siteMetadata\":{\"title\":\"Shang-Yu Su\",\"subtitle\":\"PhD student in National Taiwan University, research interests cover Deep Learning, Natural Language Processing, and dialogue systems.\",\"copyright\":\"© All rights reserved.\",\"author\":{\"name\":\"Shang-Yu Su\",\"twitter\":\"#\"},\"disqusShortname\":\"\",\"url\":\"https://www.shangyusu.com\"}},\"markdownRemark\":{\"id\":\"/Users/ShangYu/Desktop/github/shangyusu.github.io/src/pages/articles/ICASSP2019-comVAE/index.md absPath of file >>> MarkdownRemark\",\"html\":\"<p>Full paper:\\n<a href=\\\"/ICASSP_ComVAE-34ac75152d91d929def8cf5c13d83b62.pdf\\\" target=\\\"_blank\\\">Here</a></p>\\n<p>Amortized variational inference (AVI) enables efficient training of deep generative models to scale to large datasets. The quality of the approximate inference is determined by various reasons, such as the ability of producing proper variational parameters for each datapoint in the recognition network and whether the variational distribution matches the true posterior, etc.\\nThis paper focuses on the inference sub-optimality of variational auto-encoders (VAE), where the goal is to reduce the difference caused by amortizing the variational distribution parameters over the entire training set instead of optimizing for each training example individually, which is also known as the amortization gap.\\nThis paper extends Bayesian inference in VAE from the latent level to both latent and weight levels by adopting Bayesian neural networks (BNN) in the encoder, so that each datapoint obtains its own distribution for better modeling.\\nThe hybrid design in the proposed compound VAE is empirically demonstrated to be capable of mitigating the amortization gap.</p>\",\"fields\":{\"tagSlugs\":[\"/tags/vae/\"]},\"frontmatter\":{\"title\":\"Compound Variational Auto-Encoder\",\"tags\":[\"VAE\"],\"date\":\"2019-02-6T22:40:32.169Z\",\"description\":\"Shang-Yu Su, Shan-Wei Lin, and Yun-Nung Chen\",\"conference\":\"ICASSP2019\"}}},\"pathContext\":{\"slug\":\"/posts/icassp2019-comvae/\"}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---posts-icassp-2019-comvae-eee70bb90d8645bf9253.js","module.exports = {\"data\":{\"site\":{\"siteMetadata\":{\"title\":\"Shang-Yu Su\",\"subtitle\":\"PhD student in National Taiwan University, research interests cover Deep Learning, Natural Language Processing, and dialogue systems.\",\"copyright\":\"© All rights reserved.\",\"author\":{\"name\":\"Shang-Yu Su\",\"twitter\":\"#\"},\"disqusShortname\":\"\",\"url\":\"https://www.shangyusu.com\"}},\"markdownRemark\":{\"id\":\"/Users/ShangYu/Desktop/github/shangyusu.github.io/src/pages/articles/ICASSP2019-comVAE/index.md absPath of file >>> MarkdownRemark\",\"html\":\"<p>Full paper:\\n<a href=\\\"/ICASSP_ComVAE-34ac75152d91d929def8cf5c13d83b62.pdf\\\" target=\\\"_blank\\\">Here</a></p>\\n<p>Amortized variational inference (AVI) enables efficient training of deep generative models to scale to large datasets. The quality of the approximate inference is determined by various reasons, such as the ability of producing proper variational parameters for each datapoint in the recognition network and whether the variational distribution matches the true posterior, etc.\\nThis paper focuses on the inference sub-optimality of variational auto-encoders (VAE), where the goal is to reduce the difference caused by amortizing the variational distribution parameters over the entire training set instead of optimizing for each training example individually, which is also known as the amortization gap.\\nThis paper extends Bayesian inference in VAE from the latent level to both latent and weight levels by adopting Bayesian neural networks (BNN) in the encoder, so that each datapoint obtains its own distribution for better modeling.\\nThe hybrid design in the proposed compound VAE is empirically demonstrated to be capable of mitigating the amortization gap.</p>\",\"fields\":{\"tagSlugs\":[\"/tags/vae/\"]},\"frontmatter\":{\"title\":\"Compound Variational Auto-Encoder\",\"tags\":[\"VAE\"],\"date\":\"2019-02-6T22:40:32.169Z\",\"description\":\"Shang-Yu Su, Shan-Wei Lin, and Yun-Nung Chen\",\"conference\":\"ICASSP2019\"}}},\"pathContext\":{\"slug\":\"/posts/icassp2019-comvae/\"}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/posts-icassp-2019-comvae.json\n// module id = 428\n// module chunks = 228534894703402"],"sourceRoot":""}