webpackJsonp([0xe8ea88d02ba],{394:function(e,n){e.exports={data:{site:{siteMetadata:{title:"Shang-Yu Su",subtitle:"master's student in National Taiwan University, research interests cover Deep Learning, Natural Language Processing, and dialogue systems.",copyright:"Â© All rights reserved.",author:{name:"Shang-Yu Su",twitter:"#"},disqusShortname:"",url:"https://www.shangyusu.com"}},markdownRemark:{id:"/Users/ShangYu/Desktop/github/shangyusu.github.io/src/pages/articles/NAACL2018-NLG/index.md absPath of file >>> MarkdownRemark",html:'<p>Cite the paper:</p>\n<div class="gatsby-highlight">\n      <pre class="language-text"><code class="language-text">@inproceedings{su2018natural,\n  title={Natural Language Generation by Hierarchical Decoding with Linguistic Patterns},\n    author={Shang-Yu Su, Kai-Ling Lo, Yi-Ting Yeh, and Yun-Nung Chen},\n    booktitle={Proceedings of NAACL-HLT},\n    year={2018}\n}</code></pre>\n      </div>\n<p>Natural language generation (NLG) is a critical component in spoken dialogue systems.\nClassic NLG can be divided into two phases: </p>\n<ul>\n<li>sentence planning: deciding on the overall sentence structure, </li>\n<li>surface realization: determining specific word forms and flattening the sentence structure into a string. </li>\n</ul>\n<p>Many simple NLG models are based on recurrent neural networks (RNN) and sequence-to-sequence (seq2seq) model, which basically contains a encoder-decoder structure; these NLG models generate sentences from scratch by jointly optimizing sentence planning and surface realization using a simple cross entropy loss training criterion.</p>\n<p>However, the simple encoder-decoder architecture usually suffers from generating complex and long sentences, because the decoder has to learn all grammar and diction knowledge.</p>\n<p>This paper introduces a hierarchical decoding NLG model based on linguistic patterns in different levels, and shows that the proposed method outperforms the traditional one with a smaller model size.\nFurthermore, the design of the hierarchical decoding is flexible and easily-extendible in various NLG systems (The source code is available at <a href="https://github.com/MiuLab/HNLG">https://github.com/MiuLab/HNLG</a>).</p>',fields:{tagSlugs:["/tags/nlg/","/tags/hierarchical-decoding/","/tags/seq-2-seq/"]},frontmatter:{title:"Natural Language Generation by Hierarchical Decoding with Linguistic Patterns",tags:["NLG","Hierarchical Decoding","Seq2Seq"],date:"2018-02-02T22:40:32.169Z",description:"Shang-Yu Su*, Kai-Ling Lo*, Yi-Ting Yeh, and Yun-Nung Chen (co-first author)"}}},pathContext:{slug:"/posts/naacl2018-nlg/"}}}});
//# sourceMappingURL=path---posts-naacl-2018-nlg-32cb297615eb63d2ebd1.js.map