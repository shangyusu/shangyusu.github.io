{"version":3,"sources":["webpack:///path---publications-aaai-2019-dstc-7-avsd-30d5a5651d493feba42a.js","webpack:///./.cache/json/publications-aaai-2019-dstc-7-avsd.json"],"names":["webpackJsonp","457","module","exports","data","site","siteMetadata","title","subtitle","copyright","author","name","twitter","disqusShortname","url","markdownRemark","id","html","fields","tagSlugs","frontmatter","tags","date","description","conference","pathContext","slug"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,MAAQC,cAAgBC,MAAA,cAAAC,SAAA,uGAAAC,UAAA,yBAAAC,QAAuLC,KAAA,cAAAC,QAAA,KAAmCC,gBAAA,GAAAC,IAAA,8BAAyDC,gBAAmBC,GAAA,yIAAAC,KAAA,okCAAAC,QAAuuCC,UAAA,oCAA+CC,aAAgBb,MAAA,uEAAAc,MAAA,qBAAAC,KAAA,0BAAAC,YAAA,6FAAAC,WAAA,WAA+QC,aAAgBC,KAAA","file":"path---publications-aaai-2019-dstc-7-avsd-30d5a5651d493feba42a.js","sourcesContent":["webpackJsonp([66649143914690],{\n\n/***/ 457:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"site\":{\"siteMetadata\":{\"title\":\"Shang-Yu Su\",\"subtitle\":\"Researcher of Natural Language Processing, Information Retrieval, and Search/Recommendation Systems.\",\"copyright\":\"© All rights reserved.\",\"author\":{\"name\":\"Shang-Yu Su\",\"twitter\":\"#\"},\"disqusShortname\":\"\",\"url\":\"https://www.shangyusu.com\"}},\"markdownRemark\":{\"id\":\"/Users/ShangYu/Desktop/github/shangyusu.github.io/src/pages/articles/AAAI2019-DSTC7-track3/index.md absPath of file >>> MarkdownRemark\",\"html\":\"<p><b>This paper is published in the 7th Dialog System Technology Challenge (DSTC7) in the Proceedings of Thirty-Third AAAI Conference on Artificial Intelligence (AAAI 2019 - DSTC7).</b></p>\\n<p>Full paper:\\n<a href=\\\"/AAAI_2019___DSTC_Track_3-734c834eba9b64f63af0fae9f9030aba.pdf\\\" target=\\\"_blank\\\">Here</a></p>\\n<p>Visual question answering and visual dialogue tasks have been increasingly studied in the multimodal field towards more practical real-world scenarios.\\nA more challenging task, audio visual scene-aware dialogue (AVSD), is proposed to further advance the technologies that connect audio, vision, and language, which introduces temporal video information and dialogue interactions between a questioner and an answerer.\\nThis paper proposes an intuitive mechanism that fuses features and attention in multiple stages in order to well integrate multimodal features, and the results demonstrate its capability in the experiments.\\nAlso, we apply several state-of-the-art models in other tasks to the AVSD task, and further analyze their generalization across different tasks.</p>\",\"fields\":{\"tagSlugs\":[\"/tags/dialogue/\",\"/tags/visual/\"]},\"frontmatter\":{\"title\":\"Reactive Multi-Stage Feature Fusion for Multimodal Dialogue Modeling\",\"tags\":[\"Dialogue\",\"Visual\"],\"date\":\"2018-12-8T22:40:32.169Z\",\"description\":\"Yi-Ting Yeh, Tzu-Chuan Lin, Hsiao-Hua Cheng, Yi-Hsuan Deng, Shang-Yu Su, and Yun-Nung Chen\",\"conference\":\"DSTC7\"}}},\"pathContext\":{\"slug\":\"/publications/aaai2019-dstc7-avsd/\"}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---publications-aaai-2019-dstc-7-avsd-30d5a5651d493feba42a.js","module.exports = {\"data\":{\"site\":{\"siteMetadata\":{\"title\":\"Shang-Yu Su\",\"subtitle\":\"Researcher of Natural Language Processing, Information Retrieval, and Search/Recommendation Systems.\",\"copyright\":\"© All rights reserved.\",\"author\":{\"name\":\"Shang-Yu Su\",\"twitter\":\"#\"},\"disqusShortname\":\"\",\"url\":\"https://www.shangyusu.com\"}},\"markdownRemark\":{\"id\":\"/Users/ShangYu/Desktop/github/shangyusu.github.io/src/pages/articles/AAAI2019-DSTC7-track3/index.md absPath of file >>> MarkdownRemark\",\"html\":\"<p><b>This paper is published in the 7th Dialog System Technology Challenge (DSTC7) in the Proceedings of Thirty-Third AAAI Conference on Artificial Intelligence (AAAI 2019 - DSTC7).</b></p>\\n<p>Full paper:\\n<a href=\\\"/AAAI_2019___DSTC_Track_3-734c834eba9b64f63af0fae9f9030aba.pdf\\\" target=\\\"_blank\\\">Here</a></p>\\n<p>Visual question answering and visual dialogue tasks have been increasingly studied in the multimodal field towards more practical real-world scenarios.\\nA more challenging task, audio visual scene-aware dialogue (AVSD), is proposed to further advance the technologies that connect audio, vision, and language, which introduces temporal video information and dialogue interactions between a questioner and an answerer.\\nThis paper proposes an intuitive mechanism that fuses features and attention in multiple stages in order to well integrate multimodal features, and the results demonstrate its capability in the experiments.\\nAlso, we apply several state-of-the-art models in other tasks to the AVSD task, and further analyze their generalization across different tasks.</p>\",\"fields\":{\"tagSlugs\":[\"/tags/dialogue/\",\"/tags/visual/\"]},\"frontmatter\":{\"title\":\"Reactive Multi-Stage Feature Fusion for Multimodal Dialogue Modeling\",\"tags\":[\"Dialogue\",\"Visual\"],\"date\":\"2018-12-8T22:40:32.169Z\",\"description\":\"Yi-Ting Yeh, Tzu-Chuan Lin, Hsiao-Hua Cheng, Yi-Hsuan Deng, Shang-Yu Su, and Yun-Nung Chen\",\"conference\":\"DSTC7\"}}},\"pathContext\":{\"slug\":\"/publications/aaai2019-dstc7-avsd/\"}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/publications-aaai-2019-dstc-7-avsd.json\n// module id = 457\n// module chunks = 66649143914690"],"sourceRoot":""}