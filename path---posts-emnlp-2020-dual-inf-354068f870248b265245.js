webpackJsonp([52963816746389],{450:function(e,a){e.exports={data:{site:{siteMetadata:{title:"Shang-Yu Su",subtitle:"PhD candidate at National Taiwan University, research interests cover Natural Language Processing, and Dialogue Systems.",copyright:"Â© All rights reserved.",author:{name:"Shang-Yu Su",twitter:"#"},disqusShortname:"",url:"https://www.shangyusu.com"}},markdownRemark:{id:"/Users/ShangYu/Desktop/github/shangyusu.github.io/src/pages/articles/EMNLP2020-L2KD/index.md absPath of file >>> MarkdownRemark",html:"<p><b>This paper will be published in Findings of EMNLP, Virtual, November 16-20, 2020. ACL.</b></p>\n<p>Natural language understanding (NLU) and Natural language generation (NLG) tasks hold a strong dual relationship, where NLU aims at predicting semantic labels based on natural language utterances and NLG does the opposite.\nThe prior work mainly focused on exploiting the duality in model training in order to obtain the models with better performance.\nHowever, regarding the fast-growing scale of models in the current NLP area, sometimes we may have difficulty retraining whole NLU and NLG models.\nTo better address the issue, this paper proposes to leverage the duality in the inference stage without the need of retraining.\nThe experiments on three benchmark datasets demonstrate the effectiveness of the proposed method in both NLU and NLG, providing the great potential of practical usage.</p>",fields:{tagSlugs:["/tags/nlu/","/tags/nlg/","/tags/dialogue/"]},frontmatter:{title:"Dual Inference for Improving Language Understanding and Generation",tags:["NLU","NLG","Dialogue"],date:"2020-09-18T22:40:32.169Z",description:"Shang-Yu Su, Yung-Sung Chuang, and Yun-Nung Chen",conference:"EMNLP2020 (Findings)"}}},pathContext:{slug:"/posts/emnlp2020-dual-inf/"}}}});
//# sourceMappingURL=path---posts-emnlp-2020-dual-inf-354068f870248b265245.js.map