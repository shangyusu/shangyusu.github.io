webpackJsonp([64587682213437],{395:function(e,t){e.exports={data:{site:{siteMetadata:{title:"Shang-Yu Su",subtitle:"MS student in National Taiwan University, research interests cover deep learning, natural language processing, and dialogue system.",copyright:"Â© All rights reserved.",author:{name:"Shang-Yu Su",twitter:"#"},disqusShortname:"",url:"https://www.shangyusu.com"}},markdownRemark:{id:"/Users/ShangYu/Desktop/github/shangyusu.github.io/src/pages/articles/NAACL2018-NLU/index.md absPath of file >>> MarkdownRemark",html:'<p>Cite the paper:</p>\n<div class="gatsby-highlight">\n      <pre class="language-text"><code class="language-text">@inproceedings{su2018how,\n  title={How time matters: Learning Time-Decay Attention for Contextual Spoken Language Understanding in Dialogues},\n    author={Shang-Yu Su, Pei-Chieh Yuan, and Yun-Nung Chen},\n    booktitle={Proceedings of NAACL-HLT},\n    year={2018}\n}</code></pre>\n      </div>\n<p>Spoken language understanding (SLU) is an essential component in conversational systems. Most SLU components treats each utterance independently, and then the following components aggregate the multi-turn information in the separate phases.\nIn order to avoid error propagation and effectively utilize contexts, prior work leveraged history for contextual SLU.</p>\n<p>However, most previous models only paid attention to the related content in history utterances, ignoring their temporal information.\nIn the dialogues, it is intuitive that the most recent utterances are more important than the least recent ones, in other words, time-aware attention should be in a decaying manner.</p>\n<p>Therefore, this paper designs and investigates various types of time-decay attention on the sentence-level and speaker-level, and further proposes a flexible universal time-decay attention mechanism.\nThe experiments on the benchmark Dialogue State Tracking Challenge (DSTC4) dataset show that the proposed time-decay attention mechanisms significantly improve the state-of-the-art model for contextual understanding performance (The source code is at: <a href="https://github.com/MiuLab/E2E-Time-SLU">https://github.com/MiuLab/E2E-Time-SLU</a>).</p>',fields:{tagSlugs:["/tags/dialogue/","/tags/nlu/","/tags/attention/"]},frontmatter:{title:"How Time Matters: Learning Time-Decay Attention for Contextual Spoken Language Understanding in Dialogues",tags:["Dialogue","NLU","Attention"],date:"2018-02-03T22:40:32.169Z",description:"Shang-Yu Su, Pei-Chieh Yuan, and Yun-Nung Chen"}}},pathContext:{slug:"/posts/naacl2018-nlu/"}}}});
//# sourceMappingURL=path---posts-naacl-2018-nlu-e948ab6a2a824f2eaa3f.js.map